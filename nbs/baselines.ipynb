{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlines\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from human_eval.execution import check_correctness\n",
    "from human_eval.data import HUMAN_EVAL, stream_jsonl, write_jsonl\n",
    "\n",
    "problems = list(stream_jsonl(HUMAN_EVAL))\n",
    "executor = ThreadPoolExecutor(max_workers=5)\n",
    "\n",
    "## TODO: should be returning fraction of test cases passed, instead of 0/1\n",
    "def execute(problem, completion, timeout):\n",
    "    future = executor.submit(check_correctness, problem, completion, timeout)\n",
    "    result = future.result()\n",
    "    return result\n",
    "\n",
    "\n",
    "def stats_execute(problem, completion, timeout):\n",
    "    pre_base_str, tests = problem['test'].split('def check(candidate):\\n')\n",
    "    base_str = \"def check(candidate):\\n\"\n",
    "    split_tests = [pre_base_str + base_str + i for i in tests.split('\\n') if i != '']\n",
    "    \n",
    "    _problem = problem.copy()\n",
    "    results = []\n",
    "    for i in split_tests:\n",
    "        _problem['test'] = i\n",
    "        future = executor.submit(check_correctness, _problem, completion, timeout)\n",
    "        result = future.result()\n",
    "        results.append(result)\n",
    "    \n",
    "    \n",
    "    return {'task_id': problem['task_id'], 'pass_rate': sum([i['passed'] for i in results])/len(results)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@outlines.prompt\n",
    "def few_shots(instructions, examples, question):\n",
    "    \"\"\"{{ instructions }}\n",
    "\n",
    "    {% for example in examples %}\n",
    "    \n",
    "    Question:\n",
    "    ```\n",
    "    {{ example.prompt }}\n",
    "    ```\n",
    "    Answer:\n",
    "    ```\n",
    "    {{ example.canonical_solution }}\n",
    "    ```\n",
    "    {% endfor %}\n",
    "\n",
    "    Question:\n",
    "    ```\n",
    "    {{ question }}\n",
    "    ```\n",
    "    Answer:\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "instructions = \"Please answer the following question following the examples. Generate valid python code by indenting 4 spaces always.\"\n",
    "examples = problems[:2]\n",
    "\n",
    "problem_idx = 3\n",
    "problem = problems[problem_idx]\n",
    "question = problem['prompt']\n",
    "\n",
    "prompt = few_shots(instructions, examples, question)\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "from llama_cpp import Llama\n",
    "from math import exp, log, inf, sqrt\n",
    "import random\n",
    "\n",
    "model = Llama(\n",
    "    model_path=\"../deepseek-coder-6.7b-instruct.Q5_K_M.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=2048,\n",
    "    n_batch=256,\n",
    "    n_threads=8,\n",
    "    logits_all=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for problem in tqdm(problems[2:20]):\n",
    "    prompt = few_shots(instructions, examples, problem['prompt'])\n",
    "    output = model(prompt=prompt, max_tokens=80, temperature=0.2, stop=['```'])\n",
    "    res = output['choices'][0]['text']\n",
    "    items = [dict(task_id=problem['task_id'], completion=res)]\n",
    "    samples.extend(items)\n",
    "    # print(res)\n",
    "    # break\n",
    "\n",
    "write_jsonl(\"samples.jsonl\", samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute(problem, res, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(stream_jsonl(\"samples.jsonl_results.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(prompt=prompt, max_tokens=80, temperature=0.2, stop=['```'], samples=10)\n",
    "# res = output['choices'][0]['text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-mcts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
